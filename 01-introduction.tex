\chapter{Introduction}


HTTP is ``good enough'' for the most use cases of distributing files over the network. However, when we want to stream lots of data to multiple connected clients at once, we are starting to hit its limits. When two clients are requesting the same data, there is no mechanism in HTTP that would allow sending the data only once. Sending duplicate data has become a problem in large companies because of bandwidth capacity. Blizzard\footnote{Game company -- \url{https://www.blizzard.com/}} started to distribute video game content via a distributed solution because it was cheaper for the company and faster for players\cite{BigDataInVideoGames}. Linux's distributions use BitTorrent to transmit disk images\footnote{Image of Debian downloadable by BitTorrent \url{https://www.debian.org/CD/torrent-cd/}}.

The Bitcoin blockchain has now 242 gigabytes\footnote{A current size of bitcoin blockchain can be seen at \url{https://www.statista.com/statistics/647523/worldwide-bitcoin-blockchain-size/}}. When blockchain is processed (all its data are parsed), the size can grow twice as much. If there are multiple blockchains, then data can have few terabytes. When we are sharing blockchains data from the server for several clients, there is a big chance that multiple clients want the same data. They may be working on the same case and investigating the same wallets. So in standard solution with relational database and some HTTP server, the server has to search in all data (that can have a size of few terabytes) and transmits selected data to the client for every request. This problem happens even if a different client asks for same data after few moments/minutes. Behaviour mentioned above dramatically limits the scalability of the server.

Services that are using HTTP, often have client-server architecture, so there is also a problem with a single point of failure. If the server for some reason stops working, the client can not receive data. In a distributed file system such as IPFS, there is no such problem as one point of failure because all data are duplicated on multiple clients.

This Master thesis is divided into eight chapters. Chapter \ref{Cryptocurrencies} describes the differences between cryptocurrencies used in this project. Chapter \ref{ipfs} describes IPFS and all its layers. Design of the system created in this thesis is in Chapter \ref{Design}. The implementation of all applications that are provided with this project is in chapter \ref{Implementation}. Finally, summarization of results achieved in this work is in chapter \ref{Conclusion}.
